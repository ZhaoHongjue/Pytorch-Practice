{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [半小时学会 PyTorch Hook](https://cloud.tencent.com/developer/article/1475430)\n",
    "- [How to Use PyTorch Hooks](https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook for Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save gradients through `.retain_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.requires_grad: True\n",
      "y.requires_grad: True\n",
      "z.requires_grad: True\n",
      "w.requires_grad: True\n",
      "o.requires_grad: True\n",
      "x.grad: tensor([2., 3., 4., 5.])\n",
      "y.grad: tensor([2., 3., 4., 5.])\n",
      "z.grad: tensor([1., 3., 5., 7.])\n",
      "w.grad: tensor([2., 3., 4., 5.])\n",
      "o.grad: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4, dtype = torch.float, requires_grad = True)\n",
    "y = torch.arange(1, 5, dtype = torch.float, requires_grad = True)\n",
    "z = torch.arange(2, 6, dtype = torch.float, requires_grad = True)\n",
    "\n",
    "w = x + y\n",
    "w.retain_grad()\n",
    "\n",
    "o = w @ z\n",
    "o.retain_grad()\n",
    "o.backward()\n",
    "\n",
    "param_list = ['x', 'y', 'z', 'w', 'o']\n",
    "for param in param_list:\n",
    "    print(f'{param}.requires_grad:', eval(param).requires_grad)\n",
    "    \n",
    "for param in param_list:\n",
    "    print(f'{param}.grad:', eval(param).grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients through `.register_hook()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([2., 3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4, dtype = torch.float, requires_grad = True)\n",
    "y = torch.arange(1, 5, dtype = torch.float, requires_grad = True)\n",
    "z = torch.arange(2, 6, dtype = torch.float, requires_grad = True)\n",
    "\n",
    "# hook function\n",
    "def hook_print_grad(grad):\n",
    "    print(grad)\n",
    "    \n",
    "w = x + y\n",
    "handle_w = w.register_hook(hook_print_grad)\n",
    "\n",
    "o = w @ z\n",
    "handle_o =o.register_hook(hook_print_grad)\n",
    "\n",
    "o.backward()\n",
    "\n",
    "handle_w.remove()\n",
    "handle_o.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([ 4.,  6.,  8., 10.])\n",
      "y.grad: tensor([ 4.,  6.,  8., 10.])\n",
      "z.grad: tensor([1., 3., 5., 7.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4, dtype = torch.float, requires_grad = True)\n",
    "y = torch.arange(1, 5, dtype = torch.float, requires_grad = True)\n",
    "z = torch.arange(2, 6, dtype = torch.float, requires_grad = True)\n",
    "\n",
    "w = x + y\n",
    "handle_w = w.register_hook(lambda x: x * 2)\n",
    "o = w @ z\n",
    "o.backward()\n",
    "\n",
    "param_list = ['x', 'y', 'z']\n",
    "for param in param_list:\n",
    "    print(f'{param}.grad:', eval(param).grad)\n",
    "\n",
    "handle_w.remove()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks for `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<===================Forward Process====================>\n",
      "Linear(in_features=3, out_features=4, bias=True)\n",
      "- input: (tensor([1., 1., 1.], grad_fn=<BackwardHookFunctionBackward>),)\n",
      "- output: tensor([-0.3731, -0.3684,  0.9563,  0.1577], grad_fn=<AddBackward0>)\n",
      "ReLU()\n",
      "- input: (tensor([-0.3731, -0.3684,  0.9563,  0.1577],\n",
      "       grad_fn=<BackwardHookFunctionBackward>),)\n",
      "- output: tensor([0.0000, 0.0000, 0.9563, 0.1577], grad_fn=<ReluBackward0>)\n",
      "Linear(in_features=4, out_features=1, bias=True)\n",
      "- input: (tensor([0.0000, 0.0000, 0.9563, 0.1577],\n",
      "       grad_fn=<BackwardHookFunctionBackward>),)\n",
      "- output: tensor([-0.7612], grad_fn=<AddBackward0>)\n",
      "<===================Backward Process===================>\n",
      "Linear(in_features=4, out_features=1, bias=True)\n",
      "- grad_output: (tensor([1.]),)\n",
      "- grad_input: (tensor([ 0.2874,  0.4944, -0.3627, -0.2082]),)\n",
      "ReLU()\n",
      "- grad_output: (tensor([ 0.2874,  0.4944, -0.3627, -0.2082]),)\n",
      "- grad_input: (tensor([ 0.0000,  0.0000, -0.3627, -0.2082]),)\n",
      "Linear(in_features=3, out_features=4, bias=True)\n",
      "- grad_output: (tensor([ 0.0000,  0.0000, -0.3627, -0.2082]),)\n",
      "- grad_input: (tensor([-0.1738, -0.2761, -0.0299]),)\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(3, 4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(4, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = self.l2(self.act1(self.l1(X)))\n",
    "        return Y\n",
    "\n",
    "def hook_forward_fn(module, input, output):\n",
    "    print(module)\n",
    "    print('- input:', input)\n",
    "    print('- output:', output)\n",
    "\n",
    "def hook_backward_fn(module, grad_input, grad_output):\n",
    "    print(module) \n",
    "    print('- grad_output:', grad_output) \n",
    "    print('- grad_input:', grad_input)\n",
    "    \n",
    "model = NN()\n",
    "modules = model.named_children()\n",
    "for name, module in modules:\n",
    "    module.register_forward_hook(hook_forward_fn)\n",
    "    module.register_full_backward_hook(hook_backward_fn)\n",
    "    \n",
    "x = torch.ones((3,), requires_grad = True)\n",
    "print('<===================Forward Process====================>')\n",
    "o = model(x)\n",
    "print('<===================Backward Process===================>')\n",
    "o.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verbose Printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: torch.Size([10, 64, 112, 112])\n",
      "bn1: torch.Size([10, 64, 112, 112])\n",
      "relu: torch.Size([10, 64, 112, 112])\n",
      "maxpool: torch.Size([10, 64, 56, 56])\n",
      "layer1: torch.Size([10, 256, 56, 56])\n",
      "layer2: torch.Size([10, 512, 28, 28])\n",
      "layer3: torch.Size([10, 1024, 14, 14])\n",
      "layer4: torch.Size([10, 2048, 7, 7])\n",
      "avgpool: torch.Size([10, 2048, 1, 1])\n",
      "fc: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "class VerboseExecution(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"{layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.model(x)\n",
    "    \n",
    "from torchvision.models import resnet50\n",
    "\n",
    "verbose_resnet = VerboseExecution(resnet50())\n",
    "dummy_input = torch.ones(10, 3, 224, 224)\n",
    "\n",
    "_ = verbose_resnet(dummy_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer4': torch.Size([10, 2048, 7, 7]), 'avgpool': torch.Size([10, 2048, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Iterable, Callable\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model: nn.Module, layers: Iterable[str]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self._features = {layer: torch.empty(0) for layer in layers}\n",
    "\n",
    "        for layer_id in layers:\n",
    "            layer = dict([*self.model.named_modules()])[layer_id]\n",
    "            layer.register_forward_hook(self.save_outputs_hook(layer_id))\n",
    "\n",
    "    def save_outputs_hook(self, layer_id: str) -> Callable:\n",
    "        def fn(_, __, output):\n",
    "            self._features[layer_id] = output\n",
    "        return fn\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> Dict[str, torch.tensor]:\n",
    "        _ = self.model(x)\n",
    "        return self._features\n",
    "    \n",
    "\n",
    "resnet_features = FeatureExtractor(resnet50(), layers=[\"layer4\", \"avgpool\"])\n",
    "features = resnet_features(dummy_input)\n",
    "\n",
    "print({name: output.shape for name, output in features.items()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipper(model: nn.Module, val: float) -> nn.Module:\n",
    "    for parameter in model.parameters():\n",
    "        parameter.register_hook(lambda grad: grad.clamp_(-val, val))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
